Airflow is an orchestration tool for scheduling tasks.
It is an open source project developed by Airbnb.

# Conceptual -- How this Airflow framework implements DAG context in Python( real may differ)--
from datetime import datetime, timedelta

class DAG:
    def __init__(self, dag_id, start_date, schedule_interval, catchup):
        self.dag_id = dag_id
        self.start_date = start_date
        self.schedule_interval = schedule_interval
        self.catchup = catchup
        self.tasks = []

    def __enter__(self):
        print(f"Initializing DAG: {self.dag_id}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        print(f"Finalizing DAG: {self.dag_id}")
        print(f"Tasks registered: {[task.__name__ for task in self.tasks]}")

    def add_task(self, task_func):
        self.tasks.append(task_func)

# Simulating a task
def dummy_task():
    print("Running dummy task")

# Using the DAG context manager
with DAG(
    dag_id='file_sensor_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:
    dag.add_task(dummy_task)
------------------------------------------------------------

# Simulated FileSensor
def file_sensor(filepath, poke_interval=5, timeout=30):
    def sensor_task():
        print(f"[Sensor] Checking for file: {filepath}")
        waited = 0
        while waited < timeout:
            if os.path.exists(filepath):
                print(f"[Sensor] File found: {filepath}")
                return True
            print(f"[Sensor] File not found. Retrying in {poke_interval}s...")
            waited += poke_interval
        print(f"[Sensor] Timeout reached. File not found.")
        return False
    return sensor_task

# Simulated downstream task
def process_file():
    print("[Task] Processing the file...")

# Using the DAG context
with DAG(
    dag_id='file_sensor_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:
    sensor = file_sensor('data/input_file.csv')
    
    def task_wrapper():
        if sensor():
            process_file()
        else:
            print("[Task] Skipping file processing due to missing file.")

    dag.add_task(task_wrapper)
    dag.run()

-----------------------------------------------------------------------

## Bash Operator --

from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='list_gcs_files_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    list_gcs_files = BashOperator(
        task_id='list_gcs_files',
        bash_command='gsutil ls gs://dataproc_jobs_bucket/'
    )

## Trigger Dag Run Operator --

from airflow import DAG
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime

with DAG(
    dag_id='trigger_dag_example',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    trigger = TriggerDagRunOperator(
        task_id='trigger_prod_conditional_dag',
        trigger_dag_id='prod_conditional_branching_dag' 
    )
    trigger = TriggerDagRunOperator(
        task_id='trigger_list_gcs_files_dag',
        trigger_dag_id='list_gcs_files_dag' 
    )

## Sensor operator --


from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.dummy import DummyOperator
from datetime import datetime

with DAG(
    dag_id='file_sensor_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    wait_for_file = FileSensor(
        task_id='wait_for_file',
        filepath='gs://sensing_files',
        poke_interval=10,
        timeout=60
    )

    proceed = DummyOperator(task_id='proceed')

    wait_for_file >> proceed


## Email Operator 

from airflow import DAG
from airflow.operators.email import EmailOperator
from datetime import datetime

with DAG(
    dag_id='email_notification_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False
) as dag:

    send_email = EmailOperator(
        task_id='send_email',
        to='kumargovind2363@gmail.com',
        subject='Airflow Task Completed',
        html_content='<p>Your task has finished successfully!</p>'
    )

## Branch Operator 

âœ… How It Works
# BranchPythonOperator (decide_next_task) returns the task_id of the next task to run.
# Airflow skips the other tasks not returned by the branch.
# DummyOperator (end) is used to gracefully end the DAG.


from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime

def extract_data():
    print("Extracting data...")

def validate_data():
    print("Validating data...")

def decide_next_task():
    # Simulate validation result
    is_valid = True  # Change to False to test the other path
    if is_valid:
        return 'load_data'
    else:
        return 'send_alert'

def load_data():
    print("Loading data into database...")

def send_alert():
    print("Sending alert: Data validation failed.")

with DAG(
    dag_id='conditional_branching_example',
    start_date=datetime(2023, 1, 1),
    schedule_interval=None,
    catchup=False
) as dag:

    task1 = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data
    )

    task2 = PythonOperator(
        task_id='validate_data',
        python_callable=validate_data
    )
                                              # BranchPythonOperator (decide_next_task) returns the task_id of the next task to run.

    branch_task = BranchPythonOperator(
        task_id='decide_next_task',
        python_callable=decide_next_task
    )

    task3 = PythonOperator(
        task_id='load_data',
        python_callable=load_data
    )

    task4 = PythonOperator(
        task_id='send_alert',
        python_callable=send_alert
    )

    end = DummyOperator(task_id='end')

    # Define dependencies
    task1 >> task2 >> branch_task
    branch_task >> task3 >> end
    branch_task >> task4 >> end

## XCOM Operator --

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def start_number(**context):
   context["ti"].xcom_push(key = 'current_value', value =10)
   print("Starting number 10")

def add_five(**context):
   current_value = context["ti"].xcom_pull( key = 'current_value', task_ids = 'start_task')
   new_value = current_value + 5
   context["ti"].xcom_push(key = "current_value", value = new_value)
   print(f" Add 5: {current_value}) + 5 = {new_value} )

def multiply_five(** context):
   current_value = context["ti"].xcom_pull( Key = 'current_value', task_ids = 'add_five_task')
   new_value = current_value * 5
   context["ti"].xcom_pull( key ="current_value", value = new_value)
   print(f" Multiply 5 : {current_value} * 5 = {new_value})

with DAG( 
    dag_id = 'math_seq_dag',
    stu_data = datetime("2023-01-01"),
    schedule_interval = '@once'
    catchup = False
     ) as dag:

    start_task = PythonOperator(
               task_id = 'start_task', 
               python_callable = start_number , 
               provide_context = True)

    add_five_task= PythonOperator(
               task_id = 'add_five_task', 
               python_callable = add_five , 
               provide_context = True)

    multiply_five_task= PythonOperator(
               task_id = 'multiply_five_task', 
               python_callable = multiply_five , 
               provide_context = True)

    start_task >> add_five_task >> multiply_five_task

## Taskflow API 

from airflow import DAG
from airflow.decorators import task
from datetime import datetime

with DAG(
dag_id = "math_seq_dag_with_taskflow",
start_date = datetime(2023,1,1)
schedule_interval = '@once'
catchup = False,
) as dag:

@task
def start_number():
    initial_value = 10
    print(f"Start number : {start_number}")
    return initial_value


## Dag simulation with threading --
  
import os
import time
import json
import threading
import logging
from datetime import datetime, timedelta

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

# Load configuration from JSON file
with open('dag_config.json', 'r') as f:
    config = json.load(f)

# DAG class definition
class DAG:
    def __init__(self, dag_id, start_date, schedule_interval, catchup):
        self.dag_id = dag_id
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.schedule_interval = schedule_interval
        self.catchup = catchup
        self.tasks = []

    def __enter__(self):
        logging.info(f"[DAG INIT] DAG '{self.dag_id}' initialized.")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        logging.info(f"[DAG END] DAG '{self.dag_id}' finalized.")

    def add_task(self, task_func):
        self.tasks.append(task_func)

    def run(self):
        current_date = self.start_date
        today = datetime.now().date()

        while current_date.date() <= today:
            logging.info(f"[RUN] Executing DAG for date: {current_date.date()}")
            for task in self.tasks:
                try:
                    task()
                except Exception as e:
                    logging.error(f"[ERROR] Task failed: {e}")
            current_date += timedelta(days=1)
            if not self.catchup:
                break

# Simulated FileSensor with retries
def file_sensor(filepath, poke_interval=5, timeout=30, retries=3):
    def sensor_task():
        logging.info(f"[Sensor] Checking for file: {filepath}")
        attempt = 0
        while attempt < retries:
            waited = 0
            while waited < timeout:
                if os.path.exists(filepath):
                    logging.info(f"[Sensor] File found: {filepath}")
                    return True
                logging.info(f"[Sensor] File not found. Retrying in {poke_interval}s...")
                time.sleep(poke_interval)
                waited += poke_interval
            attempt += 1
            logging.warning(f"[Sensor] Retry {attempt}/{retries} failed.")
        logging.error(f"[Sensor] File not found after {retries} retries.")
        return False
    return sensor_task

# Simulated task functions
def read_file():
    logging.info("[Task] Reading the file...")

def transform_data():
    logging.info("[Task] Transforming data...")

def load_to_db():
    logging.info("[Task] Loading data to DB...")

# Task factory
def create_task(task_name):
    task_map = {
        "read_file": read_file,
        "transform_data": transform_data,
        "load_to_db": load_to_db
    }
    return task_map.get(task_name, lambda: logging.warning(f"[Task] Unknown task: {task_name}"))

# Task pipeline with parallel execution
def task_pipeline():
    sensor_config = config['file_sensor']
    sensor = file_sensor(
        filepath=sensor_config['filepath'],
        poke_interval=sensor_config['poke_interval'],
        timeout=sensor_config['timeout'],
        retries=sensor_config['retries']
    )

    if sensor():
        threads = []
        for task_name in config['tasks']:
            task = create_task(task_name)
            thread = threading.Thread(target=task)
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()
    else:
        logging.warning("[Pipeline] Skipping pipeline due to missing file.")

# Create and run the DAG using config
dag_config = config['dag']
with DAG(
    dag_id=dag_config['dag_id'],
    start_date=dag_config['start_date'],
    schedule_interval=dag_config['schedule_interval'],
    catchup=dag_config['catchup']
) as dag:
    dag.add_task(task_pipeline)
    dag.run()














