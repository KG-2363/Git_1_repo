PySpark --

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()


arrayData = [
        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),
        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),
        ('Robert',['CSharp',''],{'hair':'red','eye':''}),
        ('Washington',None,None),
        ('Jefferson',['1','2'],{}) ]

df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])
df.printSchema()
df.show()


%md
PySpark function explode(e: Column) is used to explode or create array or map columns to rows.
When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. 
When a map is passed, it creates two new columns one for key and one for value and each element in map split into the rows.
This will ignore elements that have null or empty. from the above example, Washington and Jefferson have null or empty values in array and map, 
hence the following snippet out does not contain these rows.

from pyspark.sql.functions import explode
df2 = df.select(df.name,explode(df.knownLanguages))
df2.printSchema()
df2.show()
root
 |-- name: string (nullable = true)
 |-- key: string (nullable = false)
 |-- value: string (nullable = true)

+-------+----+-----+
|   name| key|value|
+-------+----+-----+
|  James|hair|black|
|  James| eye|brown|
|Michael|hair|brown|
|Michael| eye| NULL|
| Robert|hair|  red|
| Robert| eye|     |
+-------+----+-----+



df3 = df.select(df.name,explode(df.properties))
df3.printSchema()
df3.show()
root
 |-- name: string (nullable = true)
 |-- key: string (nullable = false)
 |-- value: string (nullable = true)

+-------+----+-----+
|   name| key|value|
+-------+----+-----+
|  James|hair|black|
|  James| eye|brown|
|Michael|hair|brown|
|Michael| eye| NULL|
| Robert|hair|  red|
| Robert| eye|     |
+-------+----+-----+

df4 = df.select(df.name,explode(df.knownLanguages),explode(df.properties))
df4.printSchema()
df4.show()

root
 |-- name: string (nullable = true)
 |-- col: string (nullable = true)
 |-- key: string (nullable = false)
 |-- value: string (nullable = true)

+-------+------+----+-----+
|   name|   col| key|value|
+-------+------+----+-----+
|  James|  Java|hair|black|
|  James|  Java| eye|brown|
|  James| Scala|hair|black|
|  James| Scala| eye|brown|
|Michael| Spark|hair|brown|
|Michael| Spark| eye| NULL|
|Michael|  Java|hair|brown|
|Michael|  Java| eye| NULL|
|Michael|  NULL|hair|brown|
|Michael|  NULL| eye| NULL|
| Robert|CSharp|hair|  red|
| Robert|CSharp| eye|     |
| Robert|      |hair|  red|
| Robert|      | eye|     |
+-------+------+----+-----+


Collect_List

simpleData = [("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("James", "Sales", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000),
    ("Saif", "Sales", 4100)
  ]
schema = ["employee_name", "department", "salary"]
df = spark.createDataFrame(data=simpleData, schema = schema)
df.printSchema()
df.show(truncate=False)


root
 |-- employee_name: string (nullable = true)
 |-- department: string (nullable = true)
 |-- salary: long (nullable = true)

+-------------+----------+------+
|employee_name|department|salary|
+-------------+----------+------+
|James        |Sales     |3000  |
|Michael      |Sales     |4600  |
|Robert       |Sales     |4100  |
|Maria        |Finance   |3000  |
|James        |Sales     |3000  |
|Scott        |Finance   |3300  |
|Jen          |Finance   |3900  |
|Jeff         |Marketing |3000  |
|Kumar        |Marketing |2000  |
|Saif         |Sales     |4100  |
+-------------+----------+------+

from pyspark.sql.functions import collect_list,collect_set

df.select(collect_list("salary")).show(truncate=False)
df.show()

------------------------------------------------------------+
|collect_list(salary)                                        |
+------------------------------------------------------------+
|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|
+------------------------------------------------------------+

+-------------+----------+------+
|employee_name|department|salary|
+-------------+----------+------+
|        James|     Sales|  3000|
|      Michael|     Sales|  4600|
|       Robert|     Sales|  4100|
|        Maria|   Finance|  3000|
|        James|     Sales|  3000|
|        Scott|   Finance|  3300|
|          Jen|   Finance|  3900|
|         Jeff| Marketing|  3000|
|        Kumar| Marketing|  2000|
|         Saif|     Sales|  4100|
+-------------+----------+------+

df.select(collect_set("salary")).show(truncate=False)
+------------------------------------+
|collect_set(salary)                 |
+------------------------------------+
|[4600, 3000, 3900, 4100, 3300, 2000]|
+------------------------------------+

----------------------------------------------------------------
Can you write a word count program in pyspark from this line in text File.
from pyspark import SparkContext

sc = SparkContext()

# Sample sentence
sentence = "a black brown fox jumps over a lazy dog a black fox"

# Step 1: Split sentence into words
words = sc.parallelize(sentence.split())

# Step 2: Map each word to (word, 1)
word_pairs = words.map(lambda word: (word, 1))

# Step 3: Reduce by key (word) to count occurrences
word_counts = word_pairs.reduceByKey(lambda a, b: a + b)
--------------------------------------------------------------

You have an employee dataset with columns:
employee_id, name, salary, department.
Write a PySpark query to find the top 3 highest salaries in each department.



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, row_number
from pyspark.sql.window import Window

# Initialize Spark
spark = SparkSession.builder.appName("Top3Salaries").getOrCreate()

# Sample Data
data = [
    (1, "Alice", 90000, "IT"),
    (2, "Bob", 85000, "IT"),
    (3, "Charlie", 95000, "IT"),
    (4, "David", 70000, "HR"),
    (5, "Eva", 75000, "HR"),
    (6, "Frank", 80000, "HR"),
    (7, "Grace", 65000, "Finance"),
    (8, "Hank", 72000, "Finance"),
    (9, "Ivy", 71000, "Finance")
]

columns = ["employee_id", "name", "salary", "department"]
df = spark.createDataFrame(data, columns)

# Define Window
windowSpec = Window.partitionBy("department").orderBy(col("salary").desc())

# Apply row_number and filter top 3
result = df.withColumn("rank", row_number().over(windowSpec)) \
           .filter(col("rank") <= 3) \
           .orderBy("department", "rank")

result.show()

-----------------------------------------------------------------
Read a csv file from S3 Given a df having spaces in columns, can you transform it to have '_' instead of spaces and write to a table in database.

-- In Pyspark update col names with spaces to _
cols = df.columns
for col in cols:
  new_col = col.replace(" ","_").lower()
  df = df.withColumnRenamed(col,new_col)

# Step 4: Collect the result
print(word_counts.collect())
----------------------------------------------------------------

